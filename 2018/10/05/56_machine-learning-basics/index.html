<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/blog/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/blog/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="google5b248f7b86cbcee5.html" />














  
  
  <link href="/blog/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="machine learning,python,bioinformatics,teaching,TA," />





  <link rel="alternate" href="/blog/atom.xml" title="WonderLand" type="application/atom+xml" />






<meta name="description" content="This is one of the chapter in Bioinformatics basic course in Tsinghua University. You may also find it here The related jupyter file">
<meta name="keywords" content="machine learning,python,bioinformatics,teaching,TA">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Basics Tutorial">
<meta property="og:url" content="https://www.cmwonderland.com/blog/2018/10/05/56_machine-learning-basics/index.html">
<meta property="og:site_name" content="WonderLand">
<meta property="og:description" content="This is one of the chapter in Bioinformatics basic course in Tsinghua University. You may also find it here The related jupyter file">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://i1.fuimg.com/640680/d09e1bae36643858.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/5f2d9d3ef2c4ed55.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/dba0499f4483b826.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/2a3087cba4cad1de.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/1e71af3fee869e60.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/507c87a5d099b80e.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/4872a27f6929443d.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/6496c7878223c8e0.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/b1dcf9d78e4a221f.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/1504d9deddec2451.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/6b9145218cefa02b.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/5be83055f202d1bd.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/ac7db08435dee99a.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/1b9d375bb6fba070.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/9d2ae29ab7890051.png">
<meta property="og:updated_time" content="2019-04-23T13:07:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Basics Tutorial">
<meta name="twitter:description" content="This is one of the chapter in Bioinformatics basic course in Tsinghua University. You may also find it here The related jupyter file">
<meta name="twitter:image" content="http://i1.fuimg.com/640680/d09e1bae36643858.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.cmwonderland.com/blog/2018/10/05/56_machine-learning-basics/"/>





  <title>Machine Learning Basics Tutorial | WonderLand</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6656499bfc0e07b4e20ee4975eb85f31";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/james20141606"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_white_ffffff.png" alt="Fork me on GitHub"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WonderLand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Somnium & Somniator</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/blog/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.cmwonderland.com/blog/blog/2018/10/05/56_machine-learning-basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WonderLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine Learning Basics Tutorial</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-05T10:03:19-04:00">
                2018-10-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/blog/2018/10/05/56_machine-learning-basics/" class="leancloud_visitors" data-flag-title="Machine Learning Basics Tutorial">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  4,485
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  28
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This is one of the chapter in <a href="https://lulab.gitbooks.io/teaching/content/" target="_blank" rel="noopener">Bioinformatics basic course in Tsinghua University</a>. You may also find it <a href="https://lulab.gitbooks.io/teaching/content/part-iii.-machine-learning-basics/1.simple-machine-learning-basics.html" target="_blank" rel="noopener">here</a></p>
<p>The related <a href="https://github.com/lulab/teaching_book/blob/master/part-iii.-machine-learning-basics/1.simple-machine-learning-basics.ipynb" target="_blank" rel="noopener">jupyter file</a></p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Machine learning studies how to learn patterns from data and predict characteristics of unknown data.</p>
<p>According whether the predicted variable is known, machine learning generally fall into two categories:<br>supervised learning and unsupervised learning.</p>
<p>In supervised learning, the model takes features and class labels<br>or targer values as input to build the model. If the target variable (the variable to predict) is a<br>categorical (e.g. positive/negative), the problem is called classification. If the target variable is<br>continuous (e.g. height), the problem is called regression. Most supervised learning problems fall into<br>these two categories, however, combination of continous output and categorical output or structured output<br>are also possible.</p>
<p>In unsupervised learning, the target variables are not specified. The objective is to identify internal<br>structures (clusters) of the data. After model fitting, we can assign new samples to clusters or generate<br>samples with similar distribution as the original data. Unsupervised learning are also useful as<br>a data preprocessing step prior to supervised learning.</p>
<h2 id="Import-data"><a href="#Import-data" class="headerlink" title="Import data"></a>Import data</h2><p>Datasets for machine learning can be loaded from a variety of souces.<br>Tabular data can be loaded through the <a href="https://pandas.pydata.org" target="_blank" rel="noopener"><em>pandas</em></a> package in various formats:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Format Type</th>
<th>Data Description</th>
<th>Reader</th>
<th>Writer</th>
</tr>
</thead>
<tbody>
<tr>
<td>text</td>
<td>CSV</td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" target="_blank" rel="noopener">pandas.read_csv</a></td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html" target="_blank" rel="noopener">pandas.to_csv</a></td>
</tr>
<tr>
<td>text</td>
<td>JSON</td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html" target="_blank" rel="noopener">pandas.read_json</a></td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html" target="_blank" rel="noopener">pandas.to_json</a></td>
</tr>
<tr>
<td>text</td>
<td>HTML</td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html" target="_blank" rel="noopener">pandas.read_html</a></td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html" target="_blank" rel="noopener">pandas.to_html</a></td>
</tr>
<tr>
<td>text</td>
<td>Local clipboard</td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_clipboard.html" target="_blank" rel="noopener">pandas.read_clipboard</a></td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_clipboard.html" target="_blank" rel="noopener">pandas.to_clipboard</a></td>
</tr>
<tr>
<td>binary</td>
<td>MS Excel</td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html" target="_blank" rel="noopener">pandas.read_excel</a></td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html" target="_blank" rel="noopener">pandas.to_excel</a></td>
</tr>
<tr>
<td>binary</td>
<td>HDF5 Format</td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_hdf.html" target="_blank" rel="noopener">pandas.read_hdf</a></td>
<td><a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_hdf.html" target="_blank" rel="noopener">pandas.to_hdf</a></td>
</tr>
</tbody>
</table>
</div>
<p>You can refer to <a href="https://pandas.pydata.org/pandas-docs/stable/io.html" target="_blank" rel="noopener">Pandas IO Tools</a><br>for more usage of data importing using <em>pandas</em>.</p>
<p>For large datasets, it is recommended to use binary formats such as <em>HDF5</em> and <em>NPZ</em> for more efficient reading and writing and also reducing disk usage.</p>
<p>HDF5 format can be read to or write from numpy arrays conveniently using the <a href="http://docs.h5py.org/en/stable/" target="_blank" rel="noopener">h5py</a> package:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="comment"># read data assuming that datasets 'X' and 'y' exists in HDF5 file input_file</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(input_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    X = f[<span class="string">'X'</span>][:]</span><br><span class="line">    y = f[<span class="string">'y'</span>][:]</span><br><span class="line"><span class="comment"># write data to HDF5 file output_file</span></span><br><span class="line"><span class="comment"># X and y are numpy arrays</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(output_file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.create_dataset(<span class="string">'X'</span>, data=X)</span><br><span class="line">    f.create_dataset(<span class="string">'y'</span>, data=y)</span><br></pre></td></tr></table></figure>
<p><em>NPZ</em> format is <a href="https://docs.scipy.org/doc/numpy/reference/routines.io.html" target="_blank" rel="noopener">native format</a> for numpy. <em>NPZ/NPY</em> format can be read from file using <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html#numpy.load" target="_blank" rel="noopener">numpy.load</a> and<br>write to file using <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html#numpy.save" target="_blank" rel="noopener">numpy.save</a><br>or <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html#numpy.savez" target="_blank" rel="noopener">numpy.savez</a>.</p>
<h2 id="Import-required-Python-packages"><a href="#Import-required-Python-packages" class="headerlink" title="Import required Python packages"></a>Import required Python packages</h2><p>Documentation for required Python packages:</p>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/" target="_blank" rel="noopener">numpy</a>: arrays</li>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a>: data IO, DataFrame</li>
<li><a href="https://imbalanced-learn.readthedocs.io/en/stable" target="_blank" rel="noopener">imbalanced-learn</a>: deal with class imbalance</li>
<li><a href="http://scikit-learn.org/" target="_blank" rel="noopener">scikit-learn</a>: machine learning</li>
<li><a href="https://www.statsmodels.org/" target="_blank" rel="noopener">statsmodels</a>: statistical functions</li>
<li><a href="https://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a>: plotting</li>
<li><a href="https://matplotlib.org/" target="_blank" rel="noopener">seaborn</a>: high-level plotting based on <em>matplotlib</em></li>
<li><a href="https://jupyter.org/" target="_blank" rel="noopener">jupyter</a>: Python notebook</li>
<li><a href="https://rasbt.github.io/mlxtend" target="_blank" rel="noopener">mlxtend</a>: Extension of scikit-learn</li>
<li><a href="https://graphviz.readthedocs.io/en/stable/" target="_blank" rel="noopener">graphviz</a>: Python binding for Graphviz graph drawing software</li>
<li><a href="http://docs.wand-py.org/en/0.4.4/" target="_blank" rel="noopener">wand</a>: ImageMagick (image processing tool) binding for Python</li>
</ul>
<p>For Jupyter Notebook users, run the following magic command to display images inline.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is a magic funtion in IPython/Jupyter that import many functions and modules</span></span><br><span class="line"><span class="comment"># from matplotlib, numpy, scipy, which is roughly equivalent to:</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># import numpy as np</span></span><br><span class="line"><span class="comment"># import numpy.ma as ma</span></span><br><span class="line"><span class="comment"># import matplotlib as mpl</span></span><br><span class="line"><span class="comment"># from matplotlib import cbook, mlab, pyplot as plt</span></span><br><span class="line"><span class="comment"># from matplotlib.pyplot import *</span></span><br><span class="line"><span class="comment"># from numpy import *</span></span><br><span class="line"><span class="comment"># from numpy.fft import *</span></span><br><span class="line"><span class="comment"># from numpy.random import *</span></span><br><span class="line"><span class="comment"># from numpy.linalg import *</span></span><br><span class="line">%pylab inline</span><br></pre></td></tr></table></figure>
<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre><p>If you run Python/IPython interactively or in a script, please run the following code instead.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="comment"># For data importing</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># For machine learning</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification, make_regression, make_circles, make_moons, make_gaussian_quantiles</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, train_test_split, GridSearchCV, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, \</span><br><span class="line">    roc_curve, precision_recall_curve, average_precision_score, matthews_corrcoef, confusion_matrix</span><br><span class="line"><span class="keyword">from</span> statsmodels.robust.scale <span class="keyword">import</span> mad</span><br><span class="line"><span class="comment"># For plotting</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set()</span><br><span class="line">sns.set_style(<span class="string">'whitegrid'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> to_hex</span><br></pre></td></tr></table></figure>
<h2 id="Initialize-random-seed"><a href="#Initialize-random-seed" class="headerlink" title="Initialize random seed"></a>Initialize random seed</h2><p>We fix the random seed of numpy in this tutorial to make the results reproducible.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random_state = np.random.RandomState(<span class="number">1289237</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Generate-datasets"><a href="#Generate-datasets" class="headerlink" title="Generate datasets"></a>Generate datasets</h2><p>You can start with simple datasets that is easy to understand and visualize before handling realistic datasets.<br><em>scikit-learn</em> provides many functions (<a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets" target="_blank" rel="noopener">sklearn.datasets</a>) for generating datasets easily.</p>
<h3 id="Classification-dataset"><a href="#Classification-dataset" class="headerlink" title="Classification dataset"></a>Classification dataset</h3><p>For example, <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification" target="_blank" rel="noopener">sklearn.datasets.make_classification</a> generates samples from a mixture of Gaussian distributions with parameters to specify the number of classes,<br>number of features, number of classes, etc. The following example generate a two-class classification dataset of 1000 samples with 2 features for visualization. Samples are generated from two independent 2D Gaussian distributions. This dataset is suitable for linear classifier.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_classes=<span class="number">2</span>, n_features=<span class="number">2</span>,</span><br><span class="line">                           n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">                           random_state=random_state, class_sep=<span class="number">0.9</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> np.unique(y):</span><br><span class="line">    ax.scatter(X[y == label, <span class="number">0</span>], X[y == label, <span class="number">1</span>], s=<span class="number">10</span>, label=str(label))</span><br><span class="line">ax.legend(title=<span class="string">'Class'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/d09e1bae36643858.png" alt="Markdown"></p>
<h3 id="Regression-dataset"><a href="#Regression-dataset" class="headerlink" title="Regression dataset"></a>Regression dataset</h3><p>You can also use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" target="_blank" rel="noopener">make_regression</a> to generate a simple regression dataset.<br>The following dataset consists of 1000 samples with 1 feature and 1 response variable. A Gaussian noise 10 is added to each response variable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_regression(n_samples=<span class="number">1000</span>, n_features=<span class="number">1</span>, n_informative=<span class="number">1</span>, noise=<span class="number">10</span>, random_state=random_state)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">ax.scatter(X[:, <span class="number">0</span>], y, s=<span class="number">5</span>, label=str(label))</span><br><span class="line">ax.set_xlabel(<span class="string">'X'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'y'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/5f2d9d3ef2c4ed55.png" alt="Markdown"></p>
<h3 id="Specialized-datasets"><a href="#Specialized-datasets" class="headerlink" title="Specialized datasets"></a>Specialized datasets</h3><p><em>scikit-learn</em> also provides sample generators for specialized classification/regression/clustering problems, e.g.<br><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html" target="_blank" rel="noopener">make_circles</a>,<br><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html" target="_blank" rel="noopener">make_moons</a>,<br><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_gaussian_quantiles.html" target="_blank" rel="noopener">make_gaussian_quantiles</a>.<br>These datasets can be used to demonstrate cases where simple classifier or clustering algorithms don’t work but<br>non-linear and more complicated algorithms work better.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, f <span class="keyword">in</span> enumerate((make_circles, make_moons, make_gaussian_quantiles)):</span><br><span class="line">    <span class="keyword">if</span> f == make_gaussian_quantiles:</span><br><span class="line">        X, y = f(n_samples=<span class="number">1000</span>, random_state=random_state)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        X, y = f(n_samples=<span class="number">1000</span>, noise=<span class="number">0.03</span>,</span><br><span class="line">                 random_state=random_state)</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> np.unique(y):</span><br><span class="line">        axes[i].scatter(X[y == label, <span class="number">0</span>], X[y == label, <span class="number">1</span>], s=<span class="number">5</span>, label=str(label))</span><br><span class="line">    axes[i].legend(title=<span class="string">'Class'</span>)</span><br><span class="line">    axes[i].set_title(f.__name__)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/dba0499f4483b826.png" alt="Markdown"></p>
<h3 id="The-digits-dataset"><a href="#The-digits-dataset" class="headerlink" title="The digits dataset"></a>The <em>digits</em> dataset</h3><p><em>scikit-learn</em> also includes some commonly used public datasets that is useful for exploring machine learning algorithms in the package. For example, the <em>digits</em> dataset is a small handwriting image dataset of 10 digits.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line">X, y = load_digits(return_X_y=<span class="keyword">True</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">11</span>))</span><br><span class="line">data = np.swapaxes(X[:<span class="number">16</span>].reshape((<span class="number">-1</span>, <span class="number">8</span>, <span class="number">8</span>)), <span class="number">0</span>, <span class="number">1</span>).reshape((<span class="number">8</span>, <span class="number">-1</span>))</span><br><span class="line"><span class="keyword">with</span> plt.rc_context(&#123;<span class="string">'axes.grid'</span>: <span class="keyword">False</span>&#125;):</span><br><span class="line">    ax.imshow(data, cmap=<span class="string">'Greys'</span>)</span><br><span class="line">ax.set_axis_off()</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/2a3087cba4cad1de.png" alt="Markdown"></p>
<h3 id="Dataset-used-in-this-tutorial"><a href="#Dataset-used-in-this-tutorial" class="headerlink" title="Dataset used in this tutorial"></a>Dataset used in this tutorial</h3><p>We use <em>sklearn.datasets.make_classification</em> to generate a dataset with 2 features</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_classes=<span class="number">2</span>, n_features=<span class="number">4</span>,</span><br><span class="line">                           n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">                           class_sep=<span class="number">0.9</span>, random_state=random_state)</span><br></pre></td></tr></table></figure>
<h2 id="Single-feature-analysis"><a href="#Single-feature-analysis" class="headerlink" title="Single feature analysis"></a>Single feature analysis</h2><h3 id="Analyze-the-separability-of-classes-using-individual-features"><a href="#Analyze-the-separability-of-classes-using-individual-features" class="headerlink" title="Analyze the separability of classes using individual features"></a>Analyze the separability of classes using individual features</h3><p>Plot the distribution of feature values of each feature. A good feature should separate the two class well.<br>The following plot shows that each individual feature can largely separate the two classes, though not perfectly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, X.shape[<span class="number">1</span>], figsize=(<span class="number">15</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> (<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">        sns.kdeplot(X[y == label, i], label=str(label), ax=axes[i])</span><br><span class="line">    axes[i].legend(title=<span class="string">'class'</span>)</span><br><span class="line">    axes[i].set_xlabel(<span class="string">'Feature x[&#123;&#125;]'</span>.format(i))</span><br><span class="line">    axes[i].set_ylabel(<span class="string">'Density'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/1e71af3fee869e60.png" alt="Markdown"></p>
<h3 id="Feature-correlation-analysis"><a href="#Feature-correlation-analysis" class="headerlink" title="Feature correlation analysis"></a>Feature correlation analysis</h3><p>Sometimes highly correlated features may be detrimental to model performance and feature selection.<br>A redundant feature does not provide more information, but introduces extra parameters to the model to make<br>the model prone to overfitting. For feature selection, the model may assign a small weight to each redundant features<br>too many redundant features may dilute the contribution of individual features. Although the impact of<br>redundant features on model performance depends on the machine learning algorithm used,<br>it is a good practice to identify these features and remove/merge redundant features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = pd.DataFrame(X, columns=[<span class="string">'x[&#123;&#125;]'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])])</span><br><span class="line">data.loc[:, <span class="string">'classes'</span>] = y.astype(<span class="string">'U'</span>)</span><br><span class="line">g = sns.PairGrid(data, hue=<span class="string">'classes'</span>, vars=[<span class="string">'x[&#123;&#125;]'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])])</span><br><span class="line">g.map_offdiag(plt.scatter, s=<span class="number">3</span>)</span><br><span class="line">g.map_diag(sns.kdeplot)</span><br><span class="line">g.add_legend()</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/507c87a5d099b80e.png" alt="Markdown"></p>
<h2 id="PCA-analysis"><a href="#PCA-analysis" class="headerlink" title="PCA analysis"></a>PCA analysis</h2><p>A dataset with more than 3 features cannot be visualized directly. We can use dimension reduction<br>to embed the data on a 2D or 3D space. A dimension reduction algorithm maps data points in high dimension to low<br>dimension while preserve distance in their original space as well as possible.</p>
<p>Principal Component Analysis (PCA) is the most common algorithm for dimension reduction.<br>It maps data to a new space by linear combination of original features such that new features are linearly<br>independent and the total variance is maximized.</p>
<p>If samples can be separated well in a PCA plot, a linear classifier also works well. Otherwise,<br>a non-linear classifier may improve classification performance.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_scaled = StandardScaler().fit_transform(X)</span><br><span class="line">pca = PCA()</span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">15</span>, <span class="number">8</span>))</span><br><span class="line">axes[<span class="number">0</span>].plot(np.arange(<span class="number">0.5</span>, X.shape[<span class="number">1</span>] + <span class="number">0.5</span>), pca.explained_variance_ratio_, marker=<span class="string">'o'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xticks(np.arange(<span class="number">0.5</span>, X.shape[<span class="number">1</span>] + <span class="number">0.5</span>))</span><br><span class="line">axes[<span class="number">0</span>].set_xticklabels(np.arange(<span class="number">1</span>, X.shape[<span class="number">1</span>] + <span class="number">1</span>))</span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">'PC rank'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">'Explained variance ratio'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xlim(<span class="number">0</span>, X.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> np.unique(y):</span><br><span class="line">    axes[<span class="number">1</span>].scatter(X_pca[y == label, <span class="number">0</span>], X_pca[y == label, <span class="number">1</span>], label=label, s=<span class="number">10</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">'PC1 (&#123;:.02f&#125;%)'</span>.format(pca.explained_variance_ratio_[<span class="number">0</span>]*<span class="number">100</span>))</span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">'PC2 (&#123;:.02f&#125;%)'</span>.format(pca.explained_variance_ratio_[<span class="number">1</span>]*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/4872a27f6929443d.png" alt="Markdown"></p>
<h2 id="Data-scaling"><a href="#Data-scaling" class="headerlink" title="Data scaling"></a>Data scaling</h2><p>For most machine learning algorithms, it is recommended to scale the features to a common small scale.<br>Features of large or small scale increase the risk of numerical instability and also make the loss function<br>harder to optimize. Feature selection based on fitted coefficients of a linear model assumes that the input<br>features are in the same scale. Performance and convergence speed of gradient-based algorithms<br>such as neural networks are largely degraded if the data is not properly scaled.<br>Decision tree and random forest, however,<br>are less sensitive to data scale because they use rule-based criteria.</p>
<p>Common data scaling methods include standard/z-score scaling, min-max scaling, robust scaling and abs-max scaling.</p>
<p>Standard/z-score scaling first shift features to their centers(mean) and then divide by their standard deviation.<br>This method is suitable for most continous features of approximately Gaussian distribution.</p>
<script type="math/tex; mode=display">\text{zscore}(x_{ij}^{'}) = \frac{x_{ij} - \mu _{ij}}{\sigma _i}</script><p>Min-max scaling method scales data into range [0, 1].<br>This method is suitable for data concentrated within a range and preserves zero values for sparse data.<br>Min-max scaling is also sensitive to outliers in the data. Try removing outliers or clip data into<br>a range before scaling.</p>
<script type="math/tex; mode=display">\text{min_max}(x_{ij}^{'}) = \frac{x_{ij} - \text{min}_k \mathbf{x}_{ik}}
{\text{max}_k x_{ik} - \text{min}_k x_{ik}}</script><p>Max-abs scaling method is similar to min-max scaling, but scales data into range [-1, 1].<br>It does not shift/center the data and thus preserves signs (positive/negative) of features.<br>Like min-max, max-abs is sensitive to outliers.</p>
<script type="math/tex; mode=display">\text{max_abs}(x_{ij}^{'}) = \frac{x_{ij}}{\text{max}_k \vert x_{ik} \vert}</script><p>Robust scaling method use robust statistics (median, interquartile range) instead of mean and standard deviation.<br>Median and IQR are less sensitive to outliers.<br>For features with large numbers of outliers or largely deviates from normal distribution,<br>robust scaling is recommended.</p>
<script type="math/tex; mode=display">\text{robust_scale}(x_{ij}^{'}) = \frac{x_{ij} - \text{median}_k x_{ik}}
{Q_{0.75}(\mathbf{x}_i) - Q_{0.25}(\mathbf{x}_i)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = random_state.normal(<span class="number">10</span>, <span class="number">2</span>, size=<span class="number">1000</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">sns.distplot(x, ax=ax)</span><br><span class="line">sns.distplot(np.ravel(StandardScaler().fit_transform(x.reshape((<span class="number">-1</span>, <span class="number">1</span>)))), ax=ax)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/6496c7878223c8e0.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate features with different distribution</span></span><br><span class="line">x = np.zeros((<span class="number">1000</span>, <span class="number">4</span>))</span><br><span class="line">x[:, <span class="number">0</span>] = random_state.normal(<span class="number">10</span>, <span class="number">2</span>, size=x.shape[<span class="number">0</span>])</span><br><span class="line">x[:, <span class="number">1</span>] = random_state.gamma(shape=<span class="number">3</span>, scale=<span class="number">4</span>, size=x.shape[<span class="number">0</span>])</span><br><span class="line">x[:, <span class="number">2</span>] = random_state.poisson(<span class="number">5</span>, size=x.shape[<span class="number">0</span>])</span><br><span class="line">x[:, <span class="number">3</span>] = random_state.uniform(<span class="number">-3</span>, <span class="number">6</span>, size=x.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler</span><br><span class="line">scalers = &#123;</span><br><span class="line">    <span class="string">'Standard'</span>: StandardScaler(),</span><br><span class="line">    <span class="string">'MinMax'</span>: MinMaxScaler(),</span><br><span class="line">    <span class="string">'MaxAbs'</span>: MaxAbsScaler(),</span><br><span class="line">    <span class="string">'Robust'</span>: RobustScaler()</span><br><span class="line">&#125;</span><br><span class="line">fig, axes = plt.subplots(<span class="number">5</span>, x.shape[<span class="number">1</span>], figsize=(<span class="number">16</span>, <span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">    sns.distplot(x[:, i], ax=axes[<span class="number">0</span>, i])</span><br><span class="line">    axes[<span class="number">0</span>, i].set_title(<span class="string">'Original feature &#123;&#125;'</span>.format(i + <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> j, scaler_name <span class="keyword">in</span> enumerate(scalers.keys()):</span><br><span class="line">    x_scaled = scalers[scaler_name].fit_transform(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">        sns.distplot(x_scaled[:, i], ax=axes[j + <span class="number">1</span>, i])</span><br><span class="line">        axes[j + <span class="number">1</span>, i].set_title(<span class="string">'&#123;&#125; for feature &#123;&#125;'</span>.format(scaler_name, i + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/b1dcf9d78e4a221f.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = StandardScaler().fit_transform(X)</span><br></pre></td></tr></table></figure>
<h2 id="Split-data-into-training-and-test-set"><a href="#Split-data-into-training-and-test-set" class="headerlink" title="Split data into training and test set"></a>Split data into training and test set</h2><p>We should split the dataset into a training and test set to evaluate model performance.<br>During model training, the model overfits to the data to some extent, and so model performance<br>on the training set is generally biases and higher than on the test set. The overfitting issue<br>can be resolved by adding more independent samples to the dataset. The difference of training<br>and test performance decreases with the increase of sample size.</p>
<p>Here, we use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split</a><br>to randomly set 80% of the samples as training set and 20% as test set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=random_state)</span><br><span class="line">print(<span class="string">'number of training samples: &#123;&#125;, test samples: &#123;&#125;'</span>.format(X_train.shape[<span class="number">0</span>], X_test.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>number of training samples: 800, test samples: 200
</code></pre><h2 id="Train-the-model"><a href="#Train-the-model" class="headerlink" title="Train the model"></a>Train the model</h2><p>During model training, the parameters of the model is adjusted to minimize a loss function.</p>
<h3 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h3><p>Logistic regression is a linear model for classification. It first forms linear combination of input features<br>and then map the combined value to class probability between 0 and 1 through a non-linear sigmoid function.<br>During model training, the weights of the model are adjusted such that the cross-entropy between model prediction<br>and true labels is minimized.</p>
<script type="math/tex; mode=display">p(y_i | \mathbf{x}_i) = \frac{1}{1 + \text{exp} \left( \sum_{j=1}^M x_{ij} w_{j} + b \right)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = LogisticRegression()</span><br><span class="line">_ = model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h2 id="Model-inspection"><a href="#Model-inspection" class="headerlink" title="Model inspection"></a>Model inspection</h2><h3 id="Feature-importance"><a href="#Feature-importance" class="headerlink" title="Feature importance"></a>Feature importance</h3><p>For linear models (e.g. Logistic regression, linear regression, linear SVM), feature importance<br>is usually defined as the square of coefficients:</p>
<script type="math/tex; mode=display">\text{FeatureImportance}_j = w_{j}^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">feature_importance = np.square(np.ravel(model.coef_))</span><br><span class="line">ax.bar(np.arange(<span class="number">1</span>, X.shape[<span class="number">1</span>] + <span class="number">1</span>).astype(<span class="string">'U'</span>), feature_importance)</span><br><span class="line">ax.set_xlabel(<span class="string">'Feature'</span>)</span><br><span class="line">_ = ax.set_ylabel(<span class="string">'Feature importance'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/1504d9deddec2451.png" alt="Markdown"></p>
<h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><p>We can inspect decision boundaries of a model by predict class labels on a 2D grid of sample points.<br>You can see that the decision boundary of Logistic regression is a straight line while other classifiers<br>create non-linear and irregular decision boundaries.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">X_grid, Y_grid = np.mgrid[<span class="number">-5</span>:<span class="number">5</span>:<span class="number">0.1</span>, <span class="number">-5</span>:<span class="number">5</span>:<span class="number">0.1</span>]</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">cmap = sns.diverging_palette(<span class="number">252</span>, <span class="number">17</span>, n=<span class="number">2</span>)</span><br><span class="line">cmap = ListedColormap(cmap)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use top 2 features</span></span><br><span class="line">selected_features = np.argsort(-feature_importance)[:<span class="number">2</span>]</span><br><span class="line"><span class="keyword">for</span> n, model_class <span class="keyword">in</span> enumerate((LogisticRegression, SVC,</span><br><span class="line">                                 DecisionTreeClassifier, KNeighborsClassifier,</span><br><span class="line">                                 GaussianProcessClassifier, RandomForestClassifier)):</span><br><span class="line">    i, j = n//<span class="number">3</span>, n%<span class="number">3</span></span><br><span class="line">    model_n = model_class()</span><br><span class="line">    model_n.fit(X_train[:, selected_features], y_train)</span><br><span class="line">    labels_grid = model_n.predict(np.column_stack([np.ravel(X_grid), np.ravel(Y_grid)]))</span><br><span class="line">    </span><br><span class="line">    axes[i, j].pcolor(X_grid, Y_grid, labels_grid.reshape(X_grid.shape), </span><br><span class="line">                          cmap=cmap, linewidth=<span class="number">0</span>, edgecolor=<span class="string">'face'</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">    axes[i, j].set_title(model_class.__name__)</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> np.unique(y):</span><br><span class="line">        axes[i, j].scatter(X_train[y_train == label, selected_features[<span class="number">0</span>]],</span><br><span class="line">                           X_train[y_train == label, selected_features[<span class="number">1</span>]],</span><br><span class="line">                           s=<span class="number">3</span>, label=str(label))</span><br><span class="line">    axes[i, j].legend(title=<span class="string">'class'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/6b9145218cefa02b.png" alt="Markdown"></p>
<h2 id="Evaluate-the-model"><a href="#Evaluate-the-model" class="headerlink" title="Evaluate the model"></a>Evaluate the model</h2><h3 id="Predict-labels-on-the-test-dataset"><a href="#Predict-labels-on-the-test-dataset" class="headerlink" title="Predict labels on the test dataset"></a>Predict labels on the test dataset</h3><p>To evaluate performance of the model, we use the <em>predict</em> method of the estimator<br>to predict class labels of test data. This will return an integer array indicating class labels.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br></pre></td></tr></table></figure>
<h3 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h3><p>The most common way to evaluate classification performance is to construct a<br><a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">confusion matrix</a>.</p>
<p>A confusion matrix summarizes the number of correctly or wrongly predicted samples and is usually<br>made up of four entries:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Predicted</th>
<th>Negative</th>
<th>Positive</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>True</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Negative</strong></td>
<td>True Negative (TN)</td>
<td>False Negative (FN)</td>
</tr>
<tr>
<td><strong>Positive</strong></td>
<td>False Positive (FP)</td>
<td>True Positive (TP)</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(confusion_matrix(y_test, y_pred), </span><br><span class="line">             columns=pd.Series([<span class="string">'Negative'</span>, <span class="string">'Positive'</span>], name=<span class="string">'Predicted'</span>),</span><br><span class="line">             index=pd.Series([<span class="string">'Negative'</span>, <span class="string">'Positive'</span>], name=<span class="string">'True'</span>))</span><br></pre></td></tr></table></figure>
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted</th>
      <th>Negative</th>
      <th>Positive</th>
    </tr>
    <tr>
      <th>True</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Negative</th>
      <td>81</td>
      <td>8</td>
    </tr>
    <tr>
      <th>Positive</th>
      <td>27</td>
      <td>84</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="Evaluation-metrics-for-classification"><a href="#Evaluation-metrics-for-classification" class="headerlink" title="Evaluation metrics for classification"></a>Evaluation metrics for classification</h3><p>A variety of metrics can be calculate from entries in the confusion matrix.</p>
<p>Accuracy (0 ~ 1) summarizes both positive and negative predictions, but is biased if the classes are imbalanced:</p>
<script type="math/tex; mode=display">\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}</script><p>Recall/sensitivity (0 ~ 1) summarizes how well the model finds out positive samples:</p>
<script type="math/tex; mode=display">\text{Recall/Sensitivity} = \frac{TP}{TP + FN}</script><p>Precision/positive predictive value (0 ~ 1) summarizes how well the model finds out negative samples:</p>
<script type="math/tex; mode=display">\text{Precision/Positive Predictive Value} = \frac{TP}{TP + FP}</script><p>F1 score (0 ~ 1) balances between positive predictive value (PPV) and true positive rate (TPR) and is more suitable for<br>imbalanced dataset:</p>
<script type="math/tex; mode=display">\text{F1 score} = 2 \frac{PPV \cdot TPR}{PPV + TPR}</script><p>Matthews correlation coefficient (MCC) (-1 ~ 1) is another metric that balances between recall and precision:</p>
<script type="math/tex; mode=display">\text{MCC} = \frac{TP \times TN - FP \times FN}
{(TP + FN)(TP + FP)(TN + FP)(TN + FN)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scorers = &#123;<span class="string">'accuracy'</span>: accuracy_score,</span><br><span class="line">           <span class="string">'recall'</span>: recall_score,</span><br><span class="line">           <span class="string">'precision'</span>: precision_score,</span><br><span class="line">           <span class="string">'f1'</span>: f1_score,</span><br><span class="line">           <span class="string">'mcc'</span>: matthews_corrcoef</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> metric <span class="keyword">in</span> scorers.keys():</span><br><span class="line">    print(<span class="string">'&#123;&#125; = &#123;&#125;'</span>.format(metric, scorers[metric](y_test, y_pred)))</span><br></pre></td></tr></table></figure>
<pre><code>accuracy = 0.825
recall = 0.7567567567567568
precision = 0.9130434782608695
f1 = 0.8275862068965518
mcc = 0.6649535460625479
</code></pre><h3 id="Predict-class-probability"><a href="#Predict-class-probability" class="headerlink" title="Predict class probability"></a>Predict class probability</h3><p>Many classifiers first predict a continous value for each sample indicating confidence/probability of the prediction<br>and then choose a fixed cutoff (e.g. 0.5 for probability values) to convert the continous values to binary labels.<br>We can get the raw prediction values through the <em>predict_proba</em> method.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_score = model.predict_proba(X_test)</span><br></pre></td></tr></table></figure>
<h3 id="ROC-curve-and-precision-recall-curve"><a href="#ROC-curve-and-precision-recall-curve" class="headerlink" title="ROC curve and precision-recall curve"></a>ROC curve and precision-recall curve</h3><p>Sometimes a single fixed cutoff is insufficient to evaluate model performance.<br>Receiver Operating Characterisic (ROC) curve and Precision-Recall curve are useful tools to inspect the<br>model performance with different cutoffs. ROC curve and precision-recall curve are also less sensitive<br>to class imbalance.<br>Compared to ROC curve, precision-recall curve are more suitable for extremely imbalanced datasets.</p>
<p>The area under the ROC curve (AUROC) or average precision (AP) is a single value<br>that summarizes average model performance under different cutoffs and are very commonly used to report<br>classification performance.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">7</span>))</span><br><span class="line"><span class="comment"># ROC curve</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_score[:, <span class="number">1</span>])</span><br><span class="line">ax = axes[<span class="number">0</span>]</span><br><span class="line">ax.plot(fpr, tpr, label=<span class="string">'ROAUC = &#123;:.4f&#125;'</span>.format(roc_auc_score(y_test, y_score[:, <span class="number">1</span>])))</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], linestyle=<span class="string">'dashed'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'False positive rate'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'True positive rate'</span>)</span><br><span class="line">ax.set_title(<span class="string">'ROC curve'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"><span class="comment"># predision-recall curve</span></span><br><span class="line">precision, recall, thresholds = precision_recall_curve(y_test, y_score[:, <span class="number">1</span>])</span><br><span class="line">ax = axes[<span class="number">1</span>]</span><br><span class="line">ax.plot(precision, recall, label=<span class="string">'AP = &#123;:.4f&#125;'</span>.format(average_precision_score(y_test, y_score[:, <span class="number">1</span>])))</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], linestyle=<span class="string">'dashed'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Precision'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Precision-recall curve'</span>)</span><br><span class="line">ax.legend()</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/5be83055f202d1bd.png" alt="Markdown"></p>
<h2 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross-validation"></a>Cross-validation</h2><p>For very large datasets, a single split of the dataset into a training set and a test set is sufficient<br>to evaluate the model performance. However, for small dataset, the test samples represent only a small<br>proportion of samples in future predictions. The model performance evaluated on the test samples varies<br>greatly between resamplings of the dataset.</p>
<h3 id="K-fold-cross-validation"><a href="#K-fold-cross-validation" class="headerlink" title="K-fold cross-validation"></a>K-fold cross-validation</h3><p>Cross-validation is a commonly used technique for model evaluation on small dataset.<br>In <strong>k-fold cross-validation</strong>, the dataset is evenly divided into <em>k</em> partitions(folds).<br>In each round of validation, the model is tested on one parition and trained on remaining <em>(k-1)/k</em><br>partitions. K-fold cross-validation ensures that there is no overlap between training and test samples<br>but can have overlaps between rounds. Each sample is set as test sample for exactly once.<br>Finally, the average performance is calculated across <em>k</em> rounds.</p>
<p><em>scikit-learn</em> provides [many functions for splitting datasets]<br>(<a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection</a>).</p>
<p>Here, we use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html" target="_blank" rel="noopener">KFold</a><br>to create 10-fold cross-validation datasets. 5 and 10 are commonly used values for <em>k</em>.<br>Use 10-fold cross-validation if the sample size and computation burden permits.</p>
<p>The following code illustrates how <em>KFold</em> splits the dataset.<br>Black boxes indicates test samples in each round.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">n_splits = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">kfold = KFold(n_splits=n_splits, random_state=random_state)</span><br><span class="line">is_train = np.zeros((n_splits, X.shape[<span class="number">0</span>]), dtype=np.bool)</span><br><span class="line"><span class="keyword">for</span> i, (train_index, test_index) <span class="keyword">in</span> enumerate(kfold.split(X, y)):</span><br><span class="line">    is_train[i, train_index] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line">ax.pcolormesh(is_train)</span><br><span class="line">ax.set_yticks(np.arange(n_splits) + <span class="number">0.5</span>)</span><br><span class="line">ax.set_yticklabels(np.arange(n_splits) + <span class="number">1</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Round'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Sample'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/ac7db08435dee99a.png" alt="Markdown"></p>
<p>Then we train the model on each training set and predict labels and scores on the whole dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">predictions = np.zeros((n_splits, X.shape[<span class="number">0</span>]), dtype=np.int32)</span><br><span class="line">predicted_scores = np.zeros((n_splits, X.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_splits):</span><br><span class="line">    model.fit(X[is_train[i]], y[is_train[i]])</span><br><span class="line">    predictions[i] = model.predict(X)</span><br><span class="line">    predicted_scores[i] = model.predict_proba(X)[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="Collect-evaluation-metrics"><a href="#Collect-evaluation-metrics" class="headerlink" title="Collect evaluation metrics"></a>Collect evaluation metrics</h3><p>Next, we evaluates the model using K-fold cross-validation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cv_metrics = pd.DataFrame(np.zeros((n_splits*<span class="number">2</span>, len(scorers) + <span class="number">2</span>)),</span><br><span class="line">                          columns=list(scorers.keys()) + [<span class="string">'roc_auc'</span>, <span class="string">'average_precision'</span>])</span><br><span class="line">cv_metrics.loc[:, <span class="string">'dataset'</span>] = np.empty(n_splits*<span class="number">2</span>, dtype=<span class="string">'U'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_splits):</span><br><span class="line">    <span class="keyword">for</span> metric <span class="keyword">in</span> scorers.keys():</span><br><span class="line">        cv_metrics.loc[i*<span class="number">2</span> + <span class="number">0</span>, metric] = scorers[metric](y[is_train[i]], predictions[i, is_train[i]])</span><br><span class="line">        cv_metrics.loc[i*<span class="number">2</span> + <span class="number">1</span>, metric] = scorers[metric](y[~is_train[i]], predictions[i, ~is_train[i]])</span><br><span class="line">    cv_metrics.loc[i*<span class="number">2</span> + <span class="number">0</span>, <span class="string">'roc_auc'</span>] = roc_auc_score(y[is_train[i]], predicted_scores[i, is_train[i]])</span><br><span class="line">    cv_metrics.loc[i*<span class="number">2</span> + <span class="number">1</span>, <span class="string">'roc_auc'</span>] = roc_auc_score(y[~is_train[i]], predicted_scores[i, ~is_train[i]])</span><br><span class="line">    cv_metrics.loc[i*<span class="number">2</span> + <span class="number">0</span>, <span class="string">'average_precision'</span>] = average_precision_score(y[is_train[i]], </span><br><span class="line">                                                                           predicted_scores[i, is_train[i]])</span><br><span class="line">    cv_metrics.loc[i*<span class="number">2</span> + <span class="number">1</span>, <span class="string">'average_precision'</span>] = average_precision_score(y[~is_train[i]], </span><br><span class="line">                                                                           predicted_scores[i, ~is_train[i]])</span><br><span class="line">    cv_metrics.loc[i*<span class="number">2</span> + <span class="number">0</span>, <span class="string">'dataset'</span>] = <span class="string">'train'</span></span><br><span class="line">    cv_metrics.loc[i*<span class="number">2</span> + <span class="number">1</span>, <span class="string">'dataset'</span>] = <span class="string">'test'</span></span><br><span class="line"></span><br><span class="line">cv_metrics.head()</span><br></pre></td></tr></table></figure>
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accuracy</th>
      <th>recall</th>
      <th>precision</th>
      <th>f1</th>
      <th>mcc</th>
      <th>roc_auc</th>
      <th>average_precision</th>
      <th>dataset</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.832222</td>
      <td>0.797386</td>
      <td>0.863208</td>
      <td>0.828992</td>
      <td>0.666847</td>
      <td>0.908640</td>
      <td>0.931433</td>
      <td>train</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.810000</td>
      <td>0.809524</td>
      <td>0.755556</td>
      <td>0.781609</td>
      <td>0.614965</td>
      <td>0.882184</td>
      <td>0.844361</td>
      <td>test</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.818889</td>
      <td>0.781737</td>
      <td>0.843750</td>
      <td>0.811561</td>
      <td>0.639439</td>
      <td>0.898143</td>
      <td>0.915890</td>
      <td>train</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.900000</td>
      <td>0.961538</td>
      <td>0.862069</td>
      <td>0.909091</td>
      <td>0.804601</td>
      <td>0.985176</td>
      <td>0.988980</td>
      <td>test</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.828889</td>
      <td>0.796909</td>
      <td>0.853428</td>
      <td>0.824201</td>
      <td>0.659380</td>
      <td>0.905196</td>
      <td>0.924640</td>
      <td>train</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="Summarize-evaluate-metrics"><a href="#Summarize-evaluate-metrics" class="headerlink" title="Summarize evaluate metrics"></a>Summarize evaluate metrics</h3><p>Take average of model performance across cross-validation runs:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv_metrics_mean = cv_metrics.groupby(<span class="string">'dataset'</span>).mean()</span><br><span class="line">cv_metrics_mean</span><br></pre></td></tr></table></figure>
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accuracy</th>
      <th>recall</th>
      <th>precision</th>
      <th>f1</th>
      <th>mcc</th>
      <th>roc_auc</th>
      <th>average_precision</th>
    </tr>
    <tr>
      <th>dataset</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>test</th>
      <td>0.831000</td>
      <td>0.794919</td>
      <td>0.861380</td>
      <td>0.823425</td>
      <td>0.667898</td>
      <td>0.903903</td>
      <td>0.921943</td>
    </tr>
    <tr>
      <th>train</th>
      <td>0.833778</td>
      <td>0.795302</td>
      <td>0.862274</td>
      <td>0.827428</td>
      <td>0.669625</td>
      <td>0.906041</td>
      <td>0.924631</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plot_data = pd.melt(cv_metrics, id_vars=[<span class="string">'dataset'</span>], var_name=<span class="string">'metric'</span>, value_name=<span class="string">'value'</span>)</span><br><span class="line">sns.stripplot(x=<span class="string">'metric'</span>, y=<span class="string">'value'</span>, hue=<span class="string">'dataset'</span>, </span><br><span class="line">              dodge=<span class="keyword">True</span>, jitter=<span class="keyword">True</span>, data=plot_data, size=<span class="number">4</span>, ax=ax)</span><br><span class="line"><span class="comment">#sns.pointplot(x='metric', y='value', hue='dataset', data=plot_data, markers="d", </span></span><br><span class="line"><span class="comment">#              join=False, ci=None, ax=ax, dodge=True, palette='dark')</span></span><br><span class="line">ax.set_title(<span class="string">'Model performance using 10-fold cross-validation'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/1b9d375bb6fba070.png" alt="Markdown"></p>
<h3 id="ROC-and-PR-curves"><a href="#ROC-and-PR-curves" class="headerlink" title="ROC and PR curves"></a>ROC and PR curves</h3><p>For each cross-validation run, compute an ROC/PR curve.<br>Then plot the mean and confidence intervals across cross-validation runs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> interp</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">7</span>))</span><br><span class="line"><span class="comment"># ROC curve</span></span><br><span class="line">ax = axes[<span class="number">0</span>]</span><br><span class="line">all_fprs = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">roc_curves = np.zeros((n_splits, len(all_fprs), <span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_splits):</span><br><span class="line">    fpr, tpr, thresholds = roc_curve(y[~is_train[i]], predicted_scores[i, ~is_train[i]])</span><br><span class="line">    roc_curves[i, :, <span class="number">0</span>] = all_fprs</span><br><span class="line">    roc_curves[i, :, <span class="number">1</span>] = interp(all_fprs, fpr, tpr)</span><br><span class="line">roc_curves = pd.DataFrame(roc_curves.reshape((<span class="number">-1</span>, <span class="number">2</span>)), columns=[<span class="string">'fpr'</span>, <span class="string">'tpr'</span>])</span><br><span class="line">sns.lineplot(x=<span class="string">'fpr'</span>, y=<span class="string">'tpr'</span>, data=roc_curves, ci=<span class="string">'sd'</span>, ax=ax,</span><br><span class="line">             label=<span class="string">'Test AUC = &#123;:.4f&#125;'</span>.format(cv_metrics_mean.loc[<span class="string">'test'</span>, <span class="string">'roc_auc'</span>]))</span><br><span class="line"><span class="comment">#ax.plot(fpr, tpr, label='ROAUC = &#123;:.4f&#125;'.format(roc_auc_score(y_test, y_score[:, 1])))</span></span><br><span class="line"><span class="comment">#ax.plot([0, 1], [0, 1], linestyle='dashed')</span></span><br><span class="line">ax.set_xlabel(<span class="string">'False positive rate'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'True positive rate'</span>)</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], linestyle=<span class="string">'dashed'</span>, color=<span class="string">'gray'</span>)</span><br><span class="line">ax.set_title(<span class="string">'ROC curve'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># predision-recall curve</span></span><br><span class="line">ax = axes[<span class="number">1</span>]</span><br><span class="line">all_precs = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">pr_curves = np.zeros((n_splits, len(all_precs), <span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_splits):</span><br><span class="line">    fpr, tpr, thresholds = precision_recall_curve(y[~is_train[i]], predicted_scores[i, ~is_train[i]])</span><br><span class="line">    pr_curves[i, :, <span class="number">0</span>] = all_precs</span><br><span class="line">    pr_curves[i, :, <span class="number">1</span>] = interp(all_precs, fpr, tpr)</span><br><span class="line">pr_curves = pd.DataFrame(pr_curves.reshape((<span class="number">-1</span>, <span class="number">2</span>)), columns=[<span class="string">'precision'</span>, <span class="string">'recall'</span>])</span><br><span class="line">sns.lineplot(x=<span class="string">'precision'</span>, y=<span class="string">'recall'</span>, data=pr_curves, ci=<span class="string">'sd'</span>, ax=ax,</span><br><span class="line">             label=<span class="string">'Test AP = &#123;:.4f&#125;'</span>.format(cv_metrics_mean.loc[<span class="string">'test'</span>, <span class="string">'average_precision'</span>]))</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Precision'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], linestyle=<span class="string">'dashed'</span>, color=<span class="string">'gray'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Precision-recall curve'</span>)</span><br><span class="line">ax.legend()</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/640680/9d2ae29ab7890051.png" alt="Markdown"></p>
<h2 id="Homework"><a href="#Homework" class="headerlink" title="Homework"></a>Homework</h2><ol>
<li><p>Understand and run all code in this tutorial using Jupyter. You can generate different types of dataset or use a real dataset.</p>
</li>
<li><p>Try different classifiers (SVC, random forest, logistic regression, KNN) and compare model performance.</p>
</li>
<li><p>Try different K’s in K-fold cross-validation and compare mean and variance of model performance.</p>
</li>
<li><p>Try different class ratios and compare model performance.</p>
</li>
</ol>
<h2 id="Further-reading"><a href="#Further-reading" class="headerlink" title="Further reading"></a>Further reading</h2><h3 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h3><ol>
<li>Trevor Hastie, Robert Tibshirani, Jerome Friedman. (2009). The Elements of Statistical Learning. </li>
<li>Christopher Bishop. (2006). Pattern Recognition and Machine Learning.</li>
<li>Kevin P. Murphy. (2012). Machine Learning A Probabilisitic Perspective.</li>
<li>Sergios Theodoridis. (2009). Pattern Recognition.</li>
</ol>
<h3 id="Class-imbalance"><a href="#Class-imbalance" class="headerlink" title="Class imbalance"></a>Class imbalance</h3><ol>
<li>He, H., and Garcia, E.A. (2009). Learning from Imbalanced Data. IEEE Transactions on Knowledge and Data Engineering 21, 1263–1284.</li>
<li>Batista, G.E.A.P.A., Prati, R.C., and Monard, M.C. (2004). A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data. SIGKDD Explor. Newsl. 6, 20–29.</li>
<li>Chawla, N.V., Bowyer, K.W., Hall, L.O., and Kegelmeyer, W.P. (2002). SMOTE: Synthetic Minority Over-sampling Technique. J. Artif. Int. Res. 16, 321–357.</li>
</ol>
<h3 id="Machine-learning-in-R"><a href="#Machine-learning-in-R" class="headerlink" title="Machine learning in R"></a>Machine learning in R</h3><p>The <em>caret</em> package (a tutorial in GitBook): <a href="http://topepo.github.io/caret" target="_blank" rel="noopener">http://topepo.github.io/caret</a></p>

      
    </div>
    
    
    

    

    <div>
    
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-----The ---- end ----<i class="fa fa-paw"></i>--- Thanks --- for --- Reading----</div>
    
</div>

    
    </div>

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/machine-learning/" rel="tag"><i class="fa fa-tag"></i> machine learning</a>
          
            <a href="/blog/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
            <a href="/blog/tags/bioinformatics/" rel="tag"><i class="fa fa-tag"></i> bioinformatics</a>
          
            <a href="/blog/tags/teaching/" rel="tag"><i class="fa fa-tag"></i> teaching</a>
          
            <a href="/blog/tags/TA/" rel="tag"><i class="fa fa-tag"></i> TA</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2018/09/29/93_book_high_school/" rel="next" title="High School Experience Sharing Book">
                <i class="fa fa-chevron-left"></i> High School Experience Sharing Book
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2018/10/06/32_quiz_exrna_tutorial/" rel="prev" title="QUIZ identification of cancer biomarker from exRNA-seq data">
                QUIZ identification of cancer biomarker from exRNA-seq data <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80Njc1My8yMzI1NQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/blog/images/avatar.png"
                alt="James Chen" />
            
              <p class="site-author-name" itemprop="name">James Chen</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/blog/archives/">
              
                  <span class="site-state-item-count">93</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/blog/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/blog/tags/index.html">
                  <span class="site-state-item-count">89</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/blog/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/james20141606" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xp-chen14@mails.tsinghua.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=d9dbc8&w=a&t=n&d=MGAfMNv-Snrv0Yg1d2t2EH3ATBUCJtdgbna_qHvb4AI&co=302dad'></script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Import-data"><span class="nav-number">2.</span> <span class="nav-text">Import data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Import-required-Python-packages"><span class="nav-number">3.</span> <span class="nav-text">Import required Python packages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Initialize-random-seed"><span class="nav-number">4.</span> <span class="nav-text">Initialize random seed</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generate-datasets"><span class="nav-number">5.</span> <span class="nav-text">Generate datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification-dataset"><span class="nav-number">5.1.</span> <span class="nav-text">Classification dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression-dataset"><span class="nav-number">5.2.</span> <span class="nav-text">Regression dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Specialized-datasets"><span class="nav-number">5.3.</span> <span class="nav-text">Specialized datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-digits-dataset"><span class="nav-number">5.4.</span> <span class="nav-text">The digits dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset-used-in-this-tutorial"><span class="nav-number">5.5.</span> <span class="nav-text">Dataset used in this tutorial</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-feature-analysis"><span class="nav-number">6.</span> <span class="nav-text">Single feature analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Analyze-the-separability-of-classes-using-individual-features"><span class="nav-number">6.1.</span> <span class="nav-text">Analyze the separability of classes using individual features</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-correlation-analysis"><span class="nav-number">6.2.</span> <span class="nav-text">Feature correlation analysis</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PCA-analysis"><span class="nav-number">7.</span> <span class="nav-text">PCA analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-scaling"><span class="nav-number">8.</span> <span class="nav-text">Data scaling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Split-data-into-training-and-test-set"><span class="nav-number">9.</span> <span class="nav-text">Split data into training and test set</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-the-model"><span class="nav-number">10.</span> <span class="nav-text">Train the model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">10.1.</span> <span class="nav-text">Logistic regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-inspection"><span class="nav-number">11.</span> <span class="nav-text">Model inspection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-importance"><span class="nav-number">11.1.</span> <span class="nav-text">Feature importance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision-boundary"><span class="nav-number">11.2.</span> <span class="nav-text">Decision boundary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluate-the-model"><span class="nav-number">12.</span> <span class="nav-text">Evaluate the model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Predict-labels-on-the-test-dataset"><span class="nav-number">12.1.</span> <span class="nav-text">Predict labels on the test dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Confusion-matrix"><span class="nav-number">12.2.</span> <span class="nav-text">Confusion matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-metrics-for-classification"><span class="nav-number">12.3.</span> <span class="nav-text">Evaluation metrics for classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Predict-class-probability"><span class="nav-number">12.4.</span> <span class="nav-text">Predict class probability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ROC-curve-and-precision-recall-curve"><span class="nav-number">12.5.</span> <span class="nav-text">ROC curve and precision-recall curve</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-validation"><span class="nav-number">13.</span> <span class="nav-text">Cross-validation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-fold-cross-validation"><span class="nav-number">13.1.</span> <span class="nav-text">K-fold cross-validation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Collect-evaluation-metrics"><span class="nav-number">13.2.</span> <span class="nav-text">Collect evaluation metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarize-evaluate-metrics"><span class="nav-number">13.3.</span> <span class="nav-text">Summarize evaluate metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ROC-and-PR-curves"><span class="nav-number">13.4.</span> <span class="nav-text">ROC and PR curves</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Homework"><span class="nav-number">14.</span> <span class="nav-text">Homework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Further-reading"><span class="nav-number">15.</span> <span class="nav-text">Further reading</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Books"><span class="nav-number">15.1.</span> <span class="nav-text">Books</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Class-imbalance"><span class="nav-number">15.2.</span> <span class="nav-text">Class imbalance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-learning-in-R"><span class="nav-number">15.3.</span> <span class="nav-text">Machine learning in R</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">James Chen</span>

  
</div>



  <span class="post-meta-divider">|</span>




<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共195.6k字</span>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  



  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("klOjl0RBA8qP5IKgIXkOszBr-gzGzoHsz", "rCaN5wX4mjiRkRMzP95g7XHz");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  
</body>
</html>
